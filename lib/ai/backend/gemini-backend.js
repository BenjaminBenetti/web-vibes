/**
 * Gemini AI backend implementation
 * Provides integration with Google's Gemini API
 */
class GeminiBackend extends AIBackend {
  /**
   * Create a Gemini backend instance
   * @param {Object} config - Backend configuration
   * @param {string} config.apiUrl - Base API URL (optional, uses default)
   * @param {string} config.defaultModel - Default model to use
   */
  constructor(config = {}) {
    super("Gemini", {
      apiUrl: "https://generativelanguage.googleapis.com/v1beta",
      defaultModel: "gemini-2.5-pro",
      ...config,
    });

    this.apiKey = null;
    this.isInitialized = false;
  }

  /**
   * Initialize the backend with Gemini API credentials
   * @param {Object} credentials - Gemini credentials
   * @param {string} credentials.apiKey - Gemini API key
   * @returns {Promise<boolean>} True if initialization successful
   */
  async initialize(credentials) {
    try {
      if (!this.validateCredentials(credentials)) {
        throw new Error("Invalid Gemini credentials");
      }

      this.apiKey = credentials.apiKey;
      this.isInitialized = true;

      return true;
    } catch (error) {
      console.error("Failed to initialize Gemini backend:", error);
      this.isInitialized = false;
      return false;
    }
  }

  /**
   * Check if the backend is properly configured and ready to use
   * @returns {boolean} True if ready
   */
  isReady() {
    return this.isInitialized && !!this.apiKey;
  }

  /**
   * Send a prompt to Gemini and get a response
   * @param {AIPrompt} prompt - The prompt to send
   * @returns {Promise<AIResponse>} The AI response
   */
  async sendPrompt(prompt) {
    const startTime = Date.now();

    try {
      if (!this.isReady()) {
        throw new Error("Gemini backend not initialized");
      }

      const model = prompt.model || this.getDefaultModel();
      const requestId = this._generateRequestId();

      const requestBody = {
        contents: [
          {
            role: "user",
            parts: [
              {
                text: prompt.content,
              },
            ],
          },
        ],
        generationConfig: this._getGenerationConfig(prompt),
      };

      const response = await this._makeAPIRequest(model, requestBody);
      const processingTime = Date.now() - startTime;

      if (!response.candidates || response.candidates.length === 0) {
        throw new Error("No response generated by Gemini");
      }

      const candidate = response.candidates[0];

      // Handle different response formats from Gemini API
      let content = "";
      if (
        candidate.content &&
        candidate.content.parts &&
        candidate.content.parts.length > 0
      ) {
        content = candidate.content.parts[0].text || "";
      } else if (candidate.finishReason === "MAX_TOKENS") {
        // When hitting token limit, Gemini might not include parts
        throw new Error(
          "Response was truncated due to maximum token limit. Please try a shorter prompt or increase max tokens."
        );
      } else {
        throw new Error("Invalid response format from Gemini API");
      }

      const tokensUsed = response.usageMetadata?.totalTokenCount || 0;

      return AIResponse.createSuccess(content, {
        model,
        tokensUsed,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId,
        createdAt: new Date(),
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error("Error sending prompt to Gemini:", error);

      return AIResponse.createError(error.message, {
        model: prompt.model || this.getDefaultModel(),
        tokensUsed: 0,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId: this._generateRequestId(),
        createdAt: new Date(),
      });
    }
  }

  /**
   * Send multiple prompts in a conversation context
   * @param {Array<AIPrompt>} prompts - Array of prompts in conversation order
   * @returns {Promise<AIResponse>} The AI response to the conversation
   */
  async sendConversation(prompts) {
    const startTime = Date.now();

    try {
      if (!this.isReady()) {
        throw new Error("Gemini backend not initialized");
      }

      if (!prompts || prompts.length === 0) {
        throw new Error("No prompts provided for conversation");
      }

      const lastPrompt = prompts[prompts.length - 1];
      const model = lastPrompt.model || this.getDefaultModel();
      const requestId = this._generateRequestId();

      // Convert prompts to Gemini conversation format
      const contents = prompts.map((prompt) => ({
        role: this._convertRole(prompt.role),
        parts: [
          {
            text: prompt.content,
          },
        ],
      }));

      const requestBody = {
        contents,
        generationConfig: this._getGenerationConfig(lastPrompt),
      };

      const response = await this._makeAPIRequest(model, requestBody);
      const processingTime = Date.now() - startTime;

      if (!response.candidates || response.candidates.length === 0) {
        throw new Error("No response generated by Gemini");
      }

      const candidate = response.candidates[0];

      // Handle different response formats from Gemini API
      let content = "";
      if (
        candidate.content &&
        candidate.content.parts &&
        candidate.content.parts.length > 0
      ) {
        content = candidate.content.parts[0].text || "";
      } else if (candidate.finishReason === "MAX_TOKENS") {
        // When hitting token limit, Gemini might not include parts
        throw new Error(
          "Response was truncated due to maximum token limit. Please try a shorter prompt or increase max tokens."
        );
      } else {
        throw new Error("Invalid response format from Gemini API");
      }

      const tokensUsed = response.usageMetadata?.totalTokenCount || 0;

      return AIResponse.createSuccess(content, {
        model,
        tokensUsed,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId,
        createdAt: new Date(),
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error("Error sending conversation to Gemini:", error);

      return AIResponse.createError(error.message, {
        model: prompts[prompts.length - 1]?.model || this.getDefaultModel(),
        tokensUsed: 0,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId: this._generateRequestId(),
        createdAt: new Date(),
      });
    }
  }

  /**
   * Get available models for Gemini
   * @returns {Array<string>} Array of available model names
   */
  getAvailableModels() {
    return [
      "gemini-2.5-pro",
      "gemini-1.5-flash",
      "gemini-1.5-pro",
      "gemini-1.0-pro",
    ];
  }

  /**
   * Get the default model for Gemini
   * @returns {string} Default model name
   */
  getDefaultModel() {
    return this.config.defaultModel;
  }

  /**
   * Validate Gemini credentials
   * @param {Object} credentials - Credentials to validate
   * @returns {boolean} True if credentials are valid format
   */
  validateCredentials(credentials) {
    return (
      credentials &&
      typeof credentials.apiKey === "string" &&
      credentials.apiKey.trim().length > 0
    );
  }

  /**
   * Configure advanced generation options
   * @param {Object} options - Generation configuration options
   * @param {number} options.thinkingBudget - Thinking budget (-1 for unlimited)
   * @param {string} options.responseMimeType - Response MIME type
   */
  configureGeneration(options = {}) {
    this.config = {
      ...this.config,
      thinkingBudget: options.thinkingBudget ?? -1,
      responseMimeType: options.responseMimeType ?? "text/plain",
    };
  }

  /**
   * Get generation configuration for API requests
   * @private
   * @param {AIPrompt} prompt - The prompt to configure for
   * @returns {Object} Generation configuration
   */
  _getGenerationConfig(prompt) {
    return {
      maxOutputTokens: prompt.maxTokens,
      temperature: prompt.temperature,
      thinkingConfig: {
        thinkingBudget: this.config.thinkingBudget ?? -1,
      },
      responseMimeType: this.config.responseMimeType ?? "text/plain",
    };
  }

  /**
   * Make an API request to Gemini
   * @private
   * @param {string} model - Model to use
   * @param {Object} requestBody - Request body
   * @returns {Promise<Object>} API response
   */
  async _makeAPIRequest(model, requestBody) {
    const url = `${this.config.apiUrl}/models/${model}:generateContent?key=${this.apiKey}`;

    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      let errorData;
      try {
        errorData = await response.json();
        const errorMessage = this._parseGeminiError(errorData);
        throw new Error(
          `Gemini API error (${response.status}): ${errorMessage}`
        );
      } catch (parseError) {
        // If JSON parsing fails, fall back to text
        errorData = await response.text();
        throw new Error(`Gemini API error (${response.status}): ${errorData}`);
      }
    }

    return await response.json();
  }

  /**
   * Convert our role format to Gemini's role format
   * @private
   * @param {string} role - Our role format
   * @returns {string} Gemini role format
   */
  _convertRole(role) {
    const roleMap = {
      user: role,
      assistant: "model",
      system: "user", // Gemini doesn't have a system role, treat as user
    };

    return roleMap[role] || "user";
  }

  /**
   * Generate a unique request ID
   * @private
   * @returns {string} Unique request ID
   */
  _generateRequestId() {
    return `gemini_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Parse Gemini API error and extract meaningful message
   * @private
   * @param {Object} errorData - Error response from API
   * @returns {string} Formatted error message
   */
  _parseGeminiError(errorData) {
    if (errorData?.error) {
      const error = errorData.error;

      // Handle specific Gemini error codes
      if (error.code === 400) {
        if (error.message?.includes("API key")) {
          return "Invalid API key. Please check your Gemini API key.";
        }
        if (error.message?.includes("quota")) {
          return "API quota exceeded. Please check your usage limits.";
        }
        if (error.message?.includes("model")) {
          return "Invalid model specified. Please use a supported Gemini model.";
        }
      }

      if (error.code === 403) {
        return "Permission denied. Please check your API key permissions.";
      }

      if (error.code === 429) {
        return "Rate limit exceeded. Please wait before making more requests.";
      }

      // Return the actual error message if available
      return error.message || error.details || `Error ${error.code}`;
    }

    return "Unknown Gemini API error";
  }
}

// Export for use in other modules
if (typeof module !== "undefined" && module.exports) {
  module.exports = GeminiBackend;
} else {
  window.GeminiBackend = GeminiBackend;
}
