/**
 * Gemini AI backend implementation
 * Provides integration with Google's Gemini API
 */
class GeminiBackend extends AIBackend {
  /**
   * Create a Gemini backend instance
   * @param {Object} config - Backend configuration
   * @param {string} config.apiUrl - Base API URL (optional, uses default)
   * @param {string} config.defaultModel - Default model to use
   */
  constructor(config = {}) {
    super("Gemini", {
      apiUrl: "https://generativelanguage.googleapis.com/v1beta",
      defaultModel: "gemini-2.5-pro",
      useStreaming: false,
      ...config,
    });

    this.apiKey = null;
    this.isInitialized = false;
  }

  /**
   * Initialize the backend with Gemini API credentials
   * @param {Object} credentials - Gemini credentials
   * @param {string} credentials.apiKey - Gemini API key
   * @returns {Promise<boolean>} True if initialization successful
   */
  async initialize(credentials) {
    try {
      if (!this.validateCredentials(credentials)) {
        throw new Error("Invalid Gemini credentials");
      }

      this.apiKey = credentials.apiKey;
      this.isInitialized = true;

      return true;
    } catch (error) {
      console.error("Failed to initialize Gemini backend:", error);
      this.isInitialized = false;
      return false;
    }
  }

  /**
   * Check if the backend is properly configured and ready to use
   * @returns {boolean} True if ready
   */
  isReady() {
    return this.isInitialized && !!this.apiKey;
  }

  /**
   * Send a prompt to Gemini and get a response
   * @param {AIPrompt} prompt - The prompt to send
   * @returns {Promise<AIResponse>} The AI response
   */
  async sendPrompt(prompt) {
    const startTime = Date.now();

    try {
      if (!this.isReady()) {
        throw new Error("Gemini backend not initialized");
      }

      const model = prompt.model || this.getDefaultModel();
      const requestId = this._generateRequestId();

      const requestBody = {
        contents: [
          {
            role: "user",
            parts: [
              {
                text: prompt.content,
              },
            ],
          },
        ],
        generationConfig: this._getGenerationConfig(prompt),
      };

      const response = await this._makeAPIRequest(
        model,
        requestBody,
        this.isStreamingEnabled()
      );
      const processingTime = Date.now() - startTime;

      if (!response.candidates || response.candidates.length === 0) {
        throw new Error("No response generated by Gemini");
      }

      const content = response.candidates[0].content.parts[0].text;
      const tokensUsed = response.usageMetadata?.totalTokenCount || 0;

      return AIResponse.createSuccess(content, {
        model,
        tokensUsed,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId,
        createdAt: new Date(),
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error("Error sending prompt to Gemini:", error);

      return AIResponse.createError(error.message, {
        model: prompt.model || this.getDefaultModel(),
        tokensUsed: 0,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId: this._generateRequestId(),
        createdAt: new Date(),
      });
    }
  }

  /**
   * Send multiple prompts in a conversation context
   * @param {Array<AIPrompt>} prompts - Array of prompts in conversation order
   * @returns {Promise<AIResponse>} The AI response to the conversation
   */
  async sendConversation(prompts) {
    const startTime = Date.now();

    try {
      if (!this.isReady()) {
        throw new Error("Gemini backend not initialized");
      }

      if (!prompts || prompts.length === 0) {
        throw new Error("No prompts provided for conversation");
      }

      const lastPrompt = prompts[prompts.length - 1];
      const model = lastPrompt.model || this.getDefaultModel();
      const requestId = this._generateRequestId();

      // Convert prompts to Gemini conversation format
      const contents = prompts.map((prompt) => ({
        role: this._convertRole(prompt.role),
        parts: [
          {
            text: prompt.content,
          },
        ],
      }));

      const requestBody = {
        contents,
        generationConfig: this._getGenerationConfig(lastPrompt),
      };

      const response = await this._makeAPIRequest(
        model,
        requestBody,
        this.isStreamingEnabled()
      );
      const processingTime = Date.now() - startTime;

      if (!response.candidates || response.candidates.length === 0) {
        throw new Error("No response generated by Gemini");
      }

      const content = response.candidates[0].content.parts[0].text;
      const tokensUsed = response.usageMetadata?.totalTokenCount || 0;

      return AIResponse.createSuccess(content, {
        model,
        tokensUsed,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId,
        createdAt: new Date(),
      });
    } catch (error) {
      const processingTime = Date.now() - startTime;
      console.error("Error sending conversation to Gemini:", error);

      return AIResponse.createError(error.message, {
        model: prompts[prompts.length - 1]?.model || this.getDefaultModel(),
        tokensUsed: 0,
        processingTimeMs: processingTime,
        provider: this.name,
        requestId: this._generateRequestId(),
        createdAt: new Date(),
      });
    }
  }

  /**
   * Get available models for Gemini
   * @returns {Array<string>} Array of available model names
   */
  getAvailableModels() {
    return [
      "gemini-2.5-pro",
      "gemini-1.5-flash",
      "gemini-1.5-pro",
      "gemini-1.0-pro",
    ];
  }

  /**
   * Get the default model for Gemini
   * @returns {string} Default model name
   */
  getDefaultModel() {
    return this.config.defaultModel;
  }

  /**
   * Validate Gemini credentials
   * @param {Object} credentials - Credentials to validate
   * @returns {boolean} True if credentials are valid format
   */
  validateCredentials(credentials) {
    return (
      credentials &&
      typeof credentials.apiKey === "string" &&
      credentials.apiKey.trim().length > 0
    );
  }

  /**
   * Test the connection with Gemini API
   * @returns {Promise<boolean>} True if connection successful
   */
  /**
   * Enable or disable streaming responses
   * @param {boolean} useStreaming - Whether to use streaming
   */
  setStreaming(useStreaming) {
    this.config.useStreaming = useStreaming;
  }

  /**
   * Check if streaming is enabled
   * @returns {boolean} True if streaming is enabled
   */
  isStreamingEnabled() {
    return this.config.useStreaming || false;
  }

  /**
   * Configure advanced generation options
   * @param {Object} options - Generation configuration options
   * @param {number} options.thinkingBudget - Thinking budget (-1 for unlimited)
   * @param {string} options.responseMimeType - Response MIME type
   * @param {boolean} options.useStreaming - Enable streaming responses
   */
  configureGeneration(options = {}) {
    this.config = {
      ...this.config,
      thinkingBudget: options.thinkingBudget ?? -1,
      responseMimeType: options.responseMimeType ?? "text/plain",
      useStreaming: options.useStreaming ?? false,
    };
  }

  /**
   * Get generation configuration for API requests
   * @private
   * @param {AIPrompt} prompt - The prompt to configure for
   * @returns {Object} Generation configuration
   */
  _getGenerationConfig(prompt) {
    return {
      maxOutputTokens: prompt.maxTokens,
      temperature: prompt.temperature,
      thinkingConfig: {
        thinkingBudget: this.config.thinkingBudget ?? -1,
      },
      responseMimeType: this.config.responseMimeType ?? "text/plain",
    };
  }

  /**
   * Make an API request to Gemini
   * @private
   * @param {string} model - Model to use
   * @param {Object} requestBody - Request body
   * @param {boolean} useStreaming - Whether to use streaming API
   * @returns {Promise<Object>} API response
   */
  async _makeAPIRequest(model, requestBody, useStreaming = false) {
    const endpoint = useStreaming ? "streamGenerateContent" : "generateContent";
    const url = `${this.config.apiUrl}/models/${model}:${endpoint}?key=${this.apiKey}`;

    const response = await fetch(url, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      let errorData;
      try {
        errorData = await response.json();
        const errorMessage = this._parseGeminiError(errorData);
        throw new Error(
          `Gemini API error (${response.status}): ${errorMessage}`
        );
      } catch (parseError) {
        // If JSON parsing fails, fall back to text
        errorData = await response.text();
        throw new Error(`Gemini API error (${response.status}): ${errorData}`);
      }
    }

    if (useStreaming) {
      // Handle streaming response
      return await this._handleStreamingResponse(response);
    } else {
      return await response.json();
    }
  }

  /**
   * Handle streaming response from Gemini API
   * @private
   * @param {Response} response - Fetch response object
   * @returns {Promise<Object>} Parsed response
   */
  async _handleStreamingResponse(response) {
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    let content = "";
    let finalResponse = null;

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");

        for (const line of lines) {
          if (line.trim() === "") continue;

          // Gemini streaming responses are JSON objects separated by newlines
          try {
            const data = JSON.parse(line);
            if (data.candidates && data.candidates[0]?.content?.parts) {
              const partText = data.candidates[0].content.parts[0]?.text || "";
              content += partText;
            }
            // Keep the last complete response for metadata
            finalResponse = data;
          } catch (parseError) {
            // Skip malformed lines
            continue;
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    // Return a response in the same format as non-streaming
    return {
      candidates: [
        {
          content: {
            parts: [{ text: content }],
          },
        },
      ],
      usageMetadata: finalResponse?.usageMetadata || {},
    };
  }

  /**
   * Convert our role format to Gemini's role format
   * @private
   * @param {string} role - Our role format
   * @returns {string} Gemini role format
   */
  _convertRole(role) {
    const roleMap = {
      user: "user",
      assistant: "model",
      system: "user", // Gemini doesn't have a system role, treat as user
    };

    return roleMap[role] || "user";
  }

  /**
   * Generate a unique request ID
   * @private
   * @returns {string} Unique request ID
   */
  _generateRequestId() {
    return `gemini_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Parse Gemini API error and extract meaningful message
   * @private
   * @param {Object} errorData - Error response from API
   * @returns {string} Formatted error message
   */
  _parseGeminiError(errorData) {
    if (errorData?.error) {
      const error = errorData.error;

      // Handle specific Gemini error codes
      if (error.code === 400) {
        if (error.message?.includes("API key")) {
          return "Invalid API key. Please check your Gemini API key.";
        }
        if (error.message?.includes("quota")) {
          return "API quota exceeded. Please check your usage limits.";
        }
        if (error.message?.includes("model")) {
          return "Invalid model specified. Please use a supported Gemini model.";
        }
      }

      if (error.code === 403) {
        return "Permission denied. Please check your API key permissions.";
      }

      if (error.code === 429) {
        return "Rate limit exceeded. Please wait before making more requests.";
      }

      // Return the actual error message if available
      return error.message || error.details || `Error ${error.code}`;
    }

    return "Unknown Gemini API error";
  }
}

// Export for use in other modules
if (typeof module !== "undefined" && module.exports) {
  module.exports = GeminiBackend;
} else {
  window.GeminiBackend = GeminiBackend;
}
